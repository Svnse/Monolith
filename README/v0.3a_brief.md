MONOLITH v0.3a — HANDOFF BRIEF
================================

For the next agent picking this up. Read kernel_contract_v1.md first. The kernel is frozen — don't touch MonoGuard, MonoDock, or MonoBridge.


WHERE WE ARE (v0.2.1a)
----------------------

Working app. Local LLM chat (GGUF/llama.cpp), Stable Diffusion, AudioCraft. Modular kernel with addon system. Operators save/restore workspace snapshots. Chat UX is polished (text selection, smooth scroll, silent UPDATE mid-gen, title locking).

GitHub: https://github.com/Svnse/Monolith


CURRENT LLM PIPELINE (what exists)
-----------------------------------

Everything happens inside LLMEngine.generate() in one flat method:

    User types → PageChat.sig_generate(prompt, thinking_mode)
        → terminal_factory wraps Task (builtin.py)
        → bridge.submit → dock.enqueue → guard.submit
        → LLMEngine.generate(payload)

Inside generate():
    1. config = payload["config"] or load_config()
    2. system_prompt = MASTER_PROMPT + behavior_tags
    3. Force system_entry at conversation_history[0]
    4. Detect UPDATE via string prefix check (hacky)
    5. Append user message to history (unless UPDATE/ephemeral)
    6. If thinking_mode: append thinking directive at end
    7. Spawn GeneratorWorker thread → stream tokens back through guard → UI

Final message array the LLM sees:
    [system: MASTER_PROMPT + tags]
    [user/assistant history...]
    [user: current message]
    [system: thinking directive — optional]

No pre-processing. No post-processing. No memory. No KV cache management.
Conversation history is in-memory only, resets between sessions.


WHAT v0.3a ADDS
----------------

### 1. LLM Pipeline Stages

Replace the flat generate() assembly with a staged pipeline in core/pipeline.py:

    PRE-PROMPT  → load world model, load memory, resolve UPDATE
    ASSEMBLY    → build message array (master prompt + world model + memory + tags + history + user msg + thinking)
    GENERATION  → llama.cpp streaming (unchanged)
    POST-GEN    → extract new memories, update world model, store history

Pipeline lives in core/, NOT in the engine. Engine receives final messages list — doesn't know about stages. Pipeline runs in terminal_factory BEFORE task submission.

### 2. World Model

Persistent context store across sessions. NOT RAG. A structured JSON doc:

    world_model.json → {identity, facts[], session_summaries[], rules[]}

Injected as [WORLD MODEL] block after MASTER_PROMPT in the ASSEMBLY stage.
POST-GEN stage optionally extracts new facts via ephemeral LLM call (background, non-blocking).
Global per-installation — operators don't snapshot it.
New file: core/world_model.py

### 3. Thinking Mode Fix

Currently boolean — Std and Ext are identical. Change to string: "off" | "std" | "ext"
Each gets a different system directive. ASSEMBLY stage handles it.

### 4. Theme System

Current: hardcoded gold-on-black (#0C0C0C, #D4AF37). No user control.
Add: ui/themes/ directory with ThemeSpec dataclass. New softer default theme. Current theme becomes an option.
Also: UI rename pass — friendlier labels ("How creative" not "Temperature"), readable module labels.

### 5. Cross-Addon Communication (Mailbox)

Addons are currently siloed. Add AddonMailbox in ui/addons/mailbox.py:
- Artifact dataclass (kind, data, source_addon, metadata)
- send(from_mod, to_mod, artifact) / receive(mod_id)
- Enables: drag image from Vision → Terminal, /attach command, /generate routes to Vision

Lives at UI level. Kernel doesn't know about it. Attached to AddonContext.

### 6. Command Router (Task Fan-Out)

One user intent → multiple engine tasks. "Generate a scene with music" → Vision task + Audio task.
Router sits ABOVE kernel. Submits individual tasks through bridge.submit() like any addon.
Kernel still processes one task per engine — unchanged.
The LLM can be the router (ephemeral intent classification call).

### 7. Installer

Standalone install_monolith.py:
- Detect GPU (nvidia-smi)
- Detect Python version
- Create venv
- Install correct torch (CUDA vs CPU)
- Install requirements
- Create launch.bat

NOT a bundled exe. That's a torch+CUDA packaging nightmare for later.

### 8. Addon Creation Protocol

Two paths:
- Vibe-coders: ask the LLM in terminal to create an addon. Protocol doc gets injected as context.
- Technical: use injector module + addon_creation_guide.md

### 9. AddonSpec Evolution

Current: id, kind, title, icon, factory
Add (all optional, defaults preserve backward compat):
- engine_type: "llm" | "vision" | "audio" | None
- accepts/emits: artifact kinds for mailbox routing
- settings_schema: auto-generate settings panel
- version: addon compatibility tracking


KEY FILES TO READ
-----------------

    core/llm_config.py          — MASTER_PROMPT, TAG_MAP, config load/save
    engine/llm.py               — LLMEngine.generate(), GeneratorWorker, conversation history
    ui/addons/builtin.py        — terminal_factory (where sig_generate connects to bridge)
    ui/addons/spec.py           — AddonSpec dataclass
    ui/addons/host.py           — launch_module() lifecycle
    ui/pages/chat.py            — PageChat (most complex UI file, handles everything chat)
    monokernel/guard.py         — MonoGuard (FROZEN — read only)
    monokernel/dock.py          — MonoDock (FROZEN — read only)
    monokernel/bridge.py        — MonoBridge (FROZEN — read only)
    README/LEGACY/kernel_contract_v1.md — the constitution


PRIORITY
--------

    1. Pipeline + World Model    — the differentiator, what nobody else has
    2. Theme + UI rename         — stops scaring away non-dev users
    3. Cross-addon mailbox       — makes modules actually useful together
    4. Installer                 — adoption gate
    5. Addon protocol            — enables community
    6. Command router            — needs mailbox first
    7. AddonSpec evolution        — needs everything above first


THINGS TO WATCH OUT FOR
------------------------

- The kernel contract is FROZEN. If you think you need to modify guard/dock/bridge, you're solving the wrong problem. Build above.
- UPDATE mid-gen detection is currently a string prefix check in generate(). The pipeline should give it a proper flag.
- Conversation history is managed in two places: engine (conversation_history list) and UI (session messages). They can drift. set_history syncs UI→engine but there's no reverse sync.
- EngineBridge has generation gating (_gen_id) that silently drops tokens from cancelled generations. Don't remove this.
- The MASTER_PROMPT says "No persistent memory unless explicitly stored." When world model ships, this line becomes the justification rather than a limitation — the world model IS the explicit store.
